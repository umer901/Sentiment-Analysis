{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Aashish\n",
      "[nltk_data]     Jai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Aashish\n",
      "[nltk_data]     Jai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Aashish\n",
      "[nltk_data]     Jai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing pandas\n",
    "import pandas as pd\n",
    "\n",
    "#Importing numpy\n",
    "import numpy as np\n",
    "\n",
    "# Importing regex\n",
    "import re\n",
    "\n",
    "#train_test_split is used to split the dataset to create train and validation dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# keras library is used to create the embedding layers, tokenizers, LSTM Model, Dense layers for classifications, padding the shorter sequences etc\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "#Since, we are working with inputs which come under the domain of natural language, NLTK (natural language tool kit)\n",
    "# is essential library for input processing.\n",
    "import nltk\n",
    "# Used to get stop words which can be removed from the inputs, in the data cleaning step.\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Using this to reduce the words in the input to their base form.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Importing word_tokenize to split the imput text into individual words/tokens.\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Does The Model Use?\n",
    "Long Short Term Memory Model (RNN)\n",
    "\n",
    "## Advantages over other models\n",
    "1. LSTMs can capture long-range dependencies and sequential patterns in the input data.\n",
    "2.  LSTMs can handle variable-length input sequences which is good for our text classification problem.\n",
    "3. LSTMs automatically learn relevant features from the sequential input, eliminating the need for manual feature engineering\n",
    "4. LSTMs are well-suited for processing textual data due to their ability to capture semantic relationships and dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ing the data\n",
    "data = pd.read_csv('./Data/train/train.csv')\n",
    "X_test = pd.read_csv(\"./Data/test/test.csv\")\n",
    "y_test = pd.read_csv(\"./Data/test_labels/test_labels.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "In this step we are pre-processing the data so that we can feed that into our model.\n",
    "1. we remove the stop words.\n",
    "2. We lemmatize the text. (Reducing the words into their base form so that words can be standardized, and all variations in the meaning of the same word can be mapped on to one base form.)\n",
    "3. Tokenize the text. (Split the text into individual words. )\n",
    "\n",
    "## Reasoning Behind Preprocessing\n",
    "\n",
    "This is an essential before we could test our model on the test inputs.\n",
    "\n",
    "The inputs are classifed into 6 categories/classes. If the input belongs to a certain class then the value for that column will be 1. Similarly if the input doesnot belong to a particular class the value in that will be 0.\n",
    "\n",
    "The rows that are being removed here have value of -1 in all their columns. The provider of the dataset explains that these rows were added sometime later and hence they have values of -1. Since, the train data set did not have any input of -1, we decided to remove those particular rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english')) # words like this, and, or, is do not contribute much to the meaning of a sentence, so removing such words is preferrable\n",
    "    lemmatizer = WordNetLemmatizer() # converting the words to their base form because there is no much difference between them, and this also makes the model less complex. Words like running are converted to run\n",
    "    words = word_tokenize(text) # converting text to individual words\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words] # removing the stop words\n",
    "    words = [re.sub(r'\\d+', '', word) for word in words] # remove numbers using regex\n",
    "    return ' '.join(words) # rejoining the split sentences\n",
    "\n",
    "# getting the labels and data\n",
    "data['comment_text'] = data['comment_text'].apply(preprocess_text)\n",
    "\n",
    "X_train = data['comment_text']\n",
    "y_train = data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "\n",
    "X_test['comment_text'] = X_test['comment_text'].apply(preprocess_text)\n",
    "X_test = X_test['comment_text']\n",
    "y_test = y_test[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "\n",
    "\n",
    "# test data had rows that were not used for testing. These had labels equated to -1. This data should not be used for predictions and is hence removed\n",
    "indices_to_remove = y_test[y_test['toxic'] == -1].index\n",
    "\n",
    "y_test = y_test.drop(indices_to_remove)\n",
    "X_test = X_test.drop(indices_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model\n",
    "\n",
    "The LSTMClassifier class employs an LSTM-based architecture for text classification. In the training phase, the fit method preprocesses the input text data using a Tokenizer converting the text to numerical data, determining the vocabulary size and maximum sequence length (so that shorter sequences can be padded to make their length equal to the longer sentence). The model architecture consists of an embedding layer, converting integers to dense vectors which are used to capture word-to-word sequential dependencies, an LSTM layer remembering the old seen toekns, and a dense layer for classification with sigmoid activation which is a very good activiation function for multi-class classification. The use of the Adam optimizer and binary cross-entropy loss is chosen for binary classification. Sequences are padded to a uniform length, addressing the requirement of neural networks for fixed-length inputs. During training, the model learns to associate toxic comment labels with the processed text data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMClassifier:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = Tokenizer() # used to convert text to numeric data that can be processed by the model \n",
    "\n",
    "    def fit(self, X, y, epochs=1, batch_size=32, validation_split=0.2):\n",
    "        self.tokenizer.fit_on_texts(X)\n",
    "        self.num_words = len(self.tokenizer.word_index) + 1 # Calculate the number of unique words in the vocabulary. This determines the number of unique tokens the model can handle.\n",
    "        self.input_length = max(len(x) for x in self.tokenizer.texts_to_sequences(X)) # the max possible input to exist in the training data. NN's require inputs of fixed length, so we need to know this variable so that we can padd the shorter sentences \n",
    "        \n",
    "        self.model = Sequential() # using this to create a simple feed forward stack of layers\n",
    "\n",
    "        self.model.add(Embedding(self.num_words, 64, input_length=self.input_length)) # using this layers so that the integers can be converted to Dense vectors which are useful for capturaing sequential dependency between words. They can be used to calculate similarity, direction etc.\n",
    "        \n",
    "        #defining the model we want to use\n",
    "        self.model.add(LSTM(64))\n",
    "        # ''' Dense layer is used for classification. The parameters include the number of output units (6, corresponding to the number of classes in this task, and the activation function 'sigmoid because it is suitable for binary classification problems. '''\n",
    "        self.model.add(Dense(6, activation='sigmoid'))\n",
    "\n",
    "        # compiling with Adam optimizer and the binary cross-entropy loss function because they are suitable for binary classification\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        X = self.tokenizer.texts_to_sequences(X)\n",
    "        X = pad_sequences(X, maxlen=self.input_length) # padding shorter sentences to be the same length as the longest input becuase NNs require inputs to be same length\n",
    "\n",
    "        # training the model\n",
    "        self.model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        X = self.tokenizer.texts_to_sequences(X) # convert text to numerical data that the model can process\n",
    "        X = pad_sequences(X, maxlen=self.input_length) # padding shorter sentences to be the same length as the longest input becuase NNs require inputs to be same length\n",
    "        return self.model.evaluate(X, y) # evaluate the model\n",
    "\n",
    "    def predict(self, text):\n",
    "        text = self.tokenizer.texts_to_sequences([text]) # convert text to numerical data that the model can process\n",
    "        text = pad_sequences(text, maxlen=self.input_length) # padding shorter sentences to be the same length as the longest input becuase NNs require inputs to be same length\n",
    "        return self.model.predict(text) # predicting the classes the text belongs to\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "We can see that the model is performing very well as both training and validation loss are low and training and validation accuracy is very high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Aashish Jai\\.conda\\envs\\MLPA1\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Aashish Jai\\.conda\\envs\\MLPA1\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Aashish Jai\\.conda\\envs\\MLPA1\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Aashish Jai\\.conda\\envs\\MLPA1\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "3990/3990 [==============================] - 3246s 813ms/step - loss: 0.0668 - accuracy: 0.9899 - val_loss: 0.0510 - val_accuracy: 0.9941\n"
     ]
    }
   ],
   "source": [
    "# Initializing and fitting the model\n",
    "lstm_classifier = LSTMClassifier()\n",
    "lstm_classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "The model performs very well, with a great accuracy, and very low loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 473s 237ms/step - loss: 0.0704 - accuracy: 0.9975\n",
      "Test loss: 0.07041707634925842\n",
      "Test accuracy: 0.9974522590637207\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "loss, accuracy = lstm_classifier.evaluate(X_test, y_test)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 739ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Toxic</th>\n",
       "      <th>Severly Toxic</th>\n",
       "      <th>Obscene</th>\n",
       "      <th>Threat</th>\n",
       "      <th>Insult</th>\n",
       "      <th>Identity Hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.96439</td>\n",
       "      <td>0.135611</td>\n",
       "      <td>0.835378</td>\n",
       "      <td>0.042641</td>\n",
       "      <td>0.74836</td>\n",
       "      <td>0.171097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Toxic  Severly Toxic   Obscene    Threat   Insult  Identity Hate\n",
       "0  0.96439       0.135611  0.835378  0.042641  0.74836       0.171097"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting labels based on a custom comment\n",
    "labels = ['Toxic', 'Severly Toxic', 'Obscene', 'Threat', 'Insult', 'Identity Hate']\n",
    "new_comment = 'You are a jerk and a liar! I hate you'\n",
    "prediction = lstm_classifier.predict(new_comment)\n",
    "pred_df = pd.DataFrame(prediction, columns=labels)\n",
    "pred_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 119ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Toxic</th>\n",
       "      <th>Severly Toxic</th>\n",
       "      <th>Obscene</th>\n",
       "      <th>Threat</th>\n",
       "      <th>Insult</th>\n",
       "      <th>Identity Hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.807801</td>\n",
       "      <td>0.028417</td>\n",
       "      <td>0.316519</td>\n",
       "      <td>0.032774</td>\n",
       "      <td>0.410591</td>\n",
       "      <td>0.064306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Toxic  Severly Toxic   Obscene    Threat    Insult  Identity Hate\n",
       "0  0.807801       0.028417  0.316519  0.032774  0.410591       0.064306"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting labels based on a custom comment\n",
    "new_comment = 'youre the a horrible human being'\n",
    "prediction = lstm_classifier.predict(new_comment)\n",
    "pred_df = pd.DataFrame(prediction, columns=labels)\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested Improvements\n",
    "If the provided test data did not contain -1’s, there would have been more data to test the\n",
    "model on. This way, we would have gotten more accurate evaluation metrics. the main problem with the dataset was that the dataset was biased towards clean data. The classes that we wanted to predict were quite low. Due to this our model wasn’t able to predict such labels with high confidence.\n",
    "To counter this issue we can take mulitple steps:\n",
    "- We could use bootstrapping which is sampling with\n",
    "replacement from observed data to get around the issue\n",
    "of class imbalance\n",
    "- Another approach could be to train the model using increments of smaller data samples with a uniform distribution for each label. We can use this method in addition to Bootstrapping. This means we will effectively have a model that will  have seen roughly equal instances for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLPA1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
