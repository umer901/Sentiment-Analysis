{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7mOF2JK3xsy"
      },
      "source": [
        "## Importing libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7bvUARP3xs2",
        "outputId": "e3b9de6a-6a1c-4a33-c888-eeb8f601042a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/kabir/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /home/kabir/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /home/kabir/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Importing pandas\n",
        "import pandas as pd\n",
        "\n",
        "#Importing numpy\n",
        "import numpy as np\n",
        "\n",
        "# Importing different functions from sklearn for future purposes.\n",
        "\n",
        "# Used for feature extraction from text, which is then used as input for the model.\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Importing the classifier model.\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Pipeline is used to assemble several steps for machine learning into a single object,\n",
        "# so that the flow can be automated.\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# The data contains inputs which belong to multiple classes, hence multi-class classifier was needed.\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Importing this to calculate and print different classification metrics such as precision, recall and f1 score.\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# To calculate the accuracy of the model in the end.\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# To calculate the loss of the model\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "#Since, we are working with inputs which come under the domain of natural language, NLTK (natural language tool kit)\n",
        "# is essential library for input processing.\n",
        "import nltk\n",
        "\n",
        "# Used to get stop words which can be removed from the inputs, in the data cleaning step.\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Using this to reduce the words in the input to their base form.\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Importing word_tokenize to split the imput text into individual words/tokens.\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1bySGiJezx2"
      },
      "source": [
        "## Advantages of model:\n",
        "- Logistic regression model is a easily understandable algorithm, hence provides simplicity.\n",
        "- Since this model does not require extensie computational resources, thus it is easy to train and is efficient.\n",
        "- Has a wide range of applications. This model has been deployed successfully in various fields such as medicine and finance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60rjkGv43xs5"
      },
      "source": [
        "## Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MekGSApU3xs5"
      },
      "outputs": [],
      "source": [
        "# Loading our data using pandas library. The data is in csv format.\n",
        "data = pd.read_csv('./Data/train/train.csv') #train data\n",
        "X_test = pd.read_csv(\"./Data/test/test.csv\") # test input data\n",
        "y_test = pd.read_csv(\"./Data/test_labels/test_labels.csv\") # test outputs data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxB0uNtw3xs6"
      },
      "source": [
        "## Preprocessing the data\n",
        "\n",
        "\n",
        "In this step we are pre-processing the data so that we can feed that into our model.\n",
        "1. we remove the stop words.\n",
        "2. We lemmatize the text. (Reducing the words into their base form so that words can be standardized, and all variations in the meaning of the same word can be mapped on to one base form.)\n",
        "3. Tokenize the text. (Split the text into individual words. )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZhoRz-43xs6"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = word_tokenize(text)\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "#Preprocessing being applied on the train inputs which are in the comment_text column.\n",
        "data['comment_text'] = data['comment_text'].apply(preprocess_text)\n",
        "\n",
        "#Creating X_train\n",
        "X_train = data['comment_text']\n",
        "\n",
        "#Creating y_train\n",
        "y_train = data[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
        "\n",
        "#Preprocessing being applied on the train inputs which are in the comment_text column.\n",
        "X_test['comment_text'] = X_test['comment_text'].apply(preprocess_text)\n",
        "\n",
        "#Creating X_test\n",
        "X_test = X_test['comment_text']\n",
        "\n",
        "#Creating y_test\n",
        "y_test = y_test[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYskfjiK3xs7"
      },
      "source": [
        "## Cleaning the dataset\n",
        "\n",
        "This is an essential before we could test our model on the test inputs.\n",
        "\n",
        "The inputs are classifed into 6 categories/classes. If the input belongs to a certain class then the value for that column will be 1. Similarly if the input doesnot belong to a particular class the value in that will be 0.\n",
        "\n",
        "The rows that are being removed here have value of -1 in all their columns. The provider of the dataset explains that these rows were added sometime later and hence they have values of -1. Since, the train data set did not have any input of -1, we decided to remove those particular rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxs_wLRK3xs7"
      },
      "outputs": [],
      "source": [
        "#Obtaining the indexes with rows with values of -1\n",
        "indices_to_remove = y_test[y_test['toxic'] == -1].index\n",
        "\n",
        "#Removing the rows from X_test\n",
        "y_test_filtered = y_test.drop(indices_to_remove)\n",
        "#Removing the rows from y_test\n",
        "X_test_filtered = X_test.drop(indices_to_remove)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zNKbuun3xs8"
      },
      "source": [
        "## Creating the pipeline\n",
        "\n",
        "To simplify multiple steps of the model, we are creating a pipeline. The dataset given to the pipeline will first go through the following steps:\n",
        "\n",
        "1. TFIDF vectoriser, for feature extraction. The output will be a feature vector.\n",
        "2. The output of the vectorizer will then be used for training of the multiclass classifier. The approach being used for classification is OneVsRest (OvR), which will help extend the scope of logistic regression classifier, as it is a binary classifer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Gs3F1ZX3xs8",
        "outputId": "7bd8d2af-d870-4902-d582-7124dc5529d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
              "                ('clf',\n",
              "                 OneVsRestClassifier(estimator=LogisticRegression(solver='sag'),\n",
              "                                     n_jobs=1))])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Define the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=1)),\n",
        "])\n",
        "\n",
        "#Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t29JJymM3xs9"
      },
      "source": [
        "## Predictions\n",
        "\n",
        "Once the model is trained we give the model our test inputs to get the predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbJ6US-H3xs9"
      },
      "outputs": [],
      "source": [
        "#Test the model\n",
        "y_pred = pipeline.predict(X_test_filtered)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUO4CudJ3xs9"
      },
      "source": [
        "## Analysis\n",
        "\n",
        "Classification report is imported from the sklearn library. This was needed so we can test our model's outputs against the gold labels. The report gives us precision, recall and f1 score for each class. We also get macro and micro averages for all the metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukbmU1cO3xs-",
        "outputId": "1f6448b4-d280-4742-e2cc-3d7b675a41e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.69      0.68      6090\n",
            "           1       0.40      0.33      0.36       367\n",
            "           2       0.77      0.61      0.68      3691\n",
            "           3       0.57      0.20      0.30       211\n",
            "           4       0.74      0.52      0.61      3427\n",
            "           5       0.68      0.24      0.35       712\n",
            "\n",
            "   micro avg       0.69      0.59      0.64     14498\n",
            "   macro avg       0.64      0.43      0.50     14498\n",
            "weighted avg       0.70      0.59      0.63     14498\n",
            " samples avg       0.06      0.05      0.06     14498\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kabir/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/home/kabir/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test_filtered, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DrVO2U4p01K"
      },
      "source": [
        "Now We Calculate the loss of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U9ycgROpya2"
      },
      "outputs": [],
      "source": [
        "# Predict the probabilities of labels from the test data\n",
        "y_pred_probs = pipeline.predict_proba(X_test_filtered)\n",
        "\n",
        "# Calculate the log loss\n",
        "loss = log_loss(y_test_filtered, y_pred_probs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J16Wa1Mf_hQU"
      },
      "source": [
        "Finally, to calculate the accuracy we will be using the accuracy_score function we have imported from sklearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKFvxL-43xs_",
        "outputId": "a138066c-9f53-4705-8032-f18a0817d8f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8979805558160617"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(y_test_filtered, y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtI-lNVz12n8",
        "outputId": "1e0a0d88-7a83-412e-ea2a-06bc99e4607b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy 0.5\n"
          ]
        }
      ],
      "source": [
        "sentences = [\"I hate black people\", \"I hate jewish people\", \"All muslims are terrorist\", \"Kill yourself, bitch\", \"Fuck you, you cunt\", \"I love god\", \"i love indian food\", \"My favourite flavour of ice-cream is chocolate\", \"God is very kind\", \"Kanye West is the greatest of all times\"]\n",
        "\n",
        "\n",
        "outputs = [[1,1,0,0,0,1],\n",
        "           [1,1,0,0,0,1],\n",
        "           [1,1,0,0,0,1],\n",
        "           [1,1,0,0,1,0],\n",
        "           [1,1,0,0,1,0],\n",
        "           [0,0,0,0,0,0],\n",
        "           [0,0,0,0,0,0],\n",
        "           [0,0,0,0,0,0],\n",
        "           [0,0,0,0,0,0],\n",
        "           [0,0,0,0,0,0],\n",
        "           ]\n",
        "for i in range(len(sentences)):\n",
        "    sentences[i] = preprocess_text(sentences[i])\n",
        "\n",
        "sentence_pred = pipeline.predict(sentences)\n",
        "\n",
        "\n",
        "print(\"accuracy\", accuracy_score(outputs, sentence_pred))\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "MLPA1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}